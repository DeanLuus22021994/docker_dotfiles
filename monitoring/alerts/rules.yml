groups:
  - name: container_alerts
    interval: 30s
    rules:
      # Container down alert
      - alert: ContainerDown
        expr: up{job="cadvisor"} == 0
        for: 2m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "Container {{ $labels.instance }} is down"
          description: "Container {{ $labels.instance }} has been down for more than 2 minutes."

      # High CPU usage alert
      - alert: HighCPUUsage
        expr: rate(container_cpu_usage_seconds_total[5m]) > 0.9
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "High CPU usage on {{ $labels.name }}"
          description: "Container {{ $labels.name }} CPU usage is above 90% for 5 minutes."

      # High memory usage alert
      - alert: HighMemoryUsage
        expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "High memory usage on {{ $labels.name }}"
          description: "Container {{ $labels.name }} memory usage is above 90% for 5 minutes."

      # Container restart alert
      - alert: ContainerRestarting
        expr: rate(container_restart_count[15m]) > 0
        for: 5m
        labels:
          severity: warning
          category: stability
        annotations:
          summary: "Container {{ $labels.name }} is restarting"
          description: "Container {{ $labels.name }} has restarted {{ $value }} times in the last 15 minutes."

  - name: database_alerts
    interval: 30s
    rules:
      # PostgreSQL down alert
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database has been down for more than 1 minute."

      # PostgreSQL too many connections
      - alert: PostgreSQLTooManyConnections
        expr: sum(pg_stat_activity_count) > 80
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "PostgreSQL has too many connections"
          description: "PostgreSQL has {{ $value }} active connections (threshold: 80)."

      # Redis down alert
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "Redis is down"
          description: "Redis cache has been down for more than 1 minute."

      # Redis high memory usage
      - alert: RedisHighMemory
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "Redis memory usage is high"
          description: "Redis is using {{ $value | humanizePercentage }} of available memory."

  - name: host_alerts
    interval: 30s
    rules:
      # High system load
      - alert: HighSystemLoad
        expr: node_load1 / count(node_cpu_seconds_total{mode="idle"}) > 1.5
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "System load is high"
          description: "System load average is {{ $value }} (threshold: 1.5 per CPU)."

      # Low disk space
      - alert: LowDiskSpace
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) < 0.1
        for: 5m
        labels:
          severity: critical
          category: capacity
        annotations:
          summary: "Low disk space on root filesystem"
          description: "Root filesystem has less than 10% space remaining: {{ $value | humanizePercentage }} free."

      # High disk I/O
      - alert: HighDiskIO
        expr: rate(node_disk_io_time_seconds_total[5m]) > 0.9
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "High disk I/O detected"
          description: "Disk I/O time is above 90% for 5 minutes on {{ $labels.device }}."

  - name: service_health_alerts
    interval: 30s
    rules:
      # Service health check failing
      - alert: ServiceHealthCheckFailing
        expr: up{job=~"prometheus|grafana|minio"} == 0
        for: 2m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "{{ $labels.job }} health check is failing"
          description: "Service {{ $labels.job }} health check has been failing for more than 2 minutes."

      # Prometheus target down
      - alert: PrometheusTargetDown
        expr: up == 0
        for: 5m
        labels:
          severity: warning
          category: monitoring
        annotations:
          summary: "Prometheus target {{ $labels.job }} is down"
          description: "Prometheus cannot scrape metrics from {{ $labels.job }} for more than 5 minutes."

      # Too many failed scrapes
      - alert: TooManyFailedScrapes
        expr: rate(prometheus_target_scrapes_exceeded_sample_limit_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
          category: monitoring
        annotations:
          summary: "Too many failed scrapes for {{ $labels.job }}"
          description: "Prometheus is experiencing {{ $value }} failed scrapes per second for {{ $labels.job }}."
