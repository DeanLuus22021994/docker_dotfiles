# docker-compose.override.yml - Local development overrides TEMPLATE
# Copy this file to docker-compose.override.yml and customize
# This file is gitignored to prevent committing local settings
# NOTE: This is an EXAMPLE file with commented sections - not a valid standalone file
# Linter warnings are expected - uncomment and modify sections as needed for your environment
# yamllint disable rule:comments-indentation

services: {}

# ---------------------------------------------------------------------------
# Example overrides (uncomment the following snippet and merge into services)
#
# services:
#   # Override PostgreSQL port for local development
#   cluster-postgres:
#     ports:
#       - "5433:5432"  # Use different port to avoid conflicts
#
#   # Mount local code for hot-reload
#   cluster-web1:
#     volumes:
#       - ./dashboard:/usr/share/nginx/html:ro
#       - ./local-dev:/app/dev
#
#   # Enable debug mode
#   cluster-dashboard:
#     environment:
#       - DEBUG=true
#       - NODE_ENV=development
#
#   # Increase resource limits for local testing
#   cluster-jupyter:
#     deploy:
#       resources:
#         limits:
#           cpus: '16'
#           memory: 16G
#
#   # Add local volume mounts
#   cluster-prometheus:
#     volumes:
#       - ./local-prometheus-data:/prometheus
#
#   # Override command for debugging
#   cluster-redis:
#     command: redis-server --loglevel debug
#
#   # GPU-accelerated workload (requires NVIDIA runtime)
#   cluster-ml-model:
#     image: tensorflow/tensorflow:latest-gpu
#     deploy:
#       resources:
#         reservations:
#           devices:
#             - driver: nvidia
#               count: all
#               capabilities: [gpu]
#     environment:
#       - NVIDIA_VISIBLE_DEVICES=all
#
#   # WebAssembly workload (requires containerd image store)
#   cluster-wasm-app:
#     image: wasmedge/example-wasi:latest
#     runtime: io.containerd.wasmedge.v1
#     platform: wasi/wasm
#
#   # Local AI inference using Docker Model Runner
#   local-inference:
#     image: ollama/ollama:latest
#     ports:
#       - "11434:11434"
#     environment:
#       - OLLAMA_HOST=0.0.0.0
#     # Access via model-runner.docker.internal:80 or localhost:12434

# Example: Add development-only services (uncomment to use)
# local-adminer:
#   image: adminer
#   ports:
#     - "8090:8080"
#   networks:
#     - cluster-network
